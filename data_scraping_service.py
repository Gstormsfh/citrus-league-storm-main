#!/usr/bin/env python3
"""
data_scraping_service.py - THE TOTAL CITRUS ENGINE (MASTER EDITION)
PARALLEL Live Sync + Automated Nightly PBP Processing (xG Audit).
Built with 100-IP rotation for TRUE real-time updates (Yahoo competitive).

RECENT FIXES (Jan 15, 2026):
- CRITICAL: Fixed unreachable code bug - nightly processing now executes properly
- Added performance tracking and health monitoring
- Improved error handling with exponential backoff
- Added detailed success/failure metrics per game
- Better logging throughout with context-aware messages
- Graceful shutdown with statistics summary
- Consecutive failure detection with alerting
"""
import os
import sys
import time
import random
import logging
import signal
import datetime as dt
from typing import Optional, Dict, Any, List, Tuple
from dotenv import load_dotenv
import requests
from concurrent.futures import ThreadPoolExecutor, as_completed
from src.utils.citrus_request import citrus_request

load_dotenv()

# High-Visibility Logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
    handlers=[logging.StreamHandler(sys.stdout)]
)
logger = logging.getLogger("CitrusMaster")

# Performance tracking
class PerformanceTracker:
    """Track service health and performance metrics"""
    def __init__(self):
        self.total_syncs = 0
        self.failed_syncs = 0
        self.games_processed = 0
        self.games_failed = 0
        self.last_sync_duration = 0
        self.service_start_time = dt.datetime.now()
    
    def log_health_check(self):
        """Log periodic health check"""
        uptime = dt.datetime.now() - self.service_start_time
        success_rate = 100 * (1 - self.failed_syncs / max(1, self.total_syncs))
        game_success_rate = 100 * (1 - self.games_failed / max(1, self.games_processed))
        
        logger.info(f"üíö HEALTH: {uptime} uptime | "
                   f"Syncs: {self.total_syncs} ({success_rate:.1f}% success) | "
                   f"Games: {self.games_processed} ({game_success_rate:.1f}% success) | "
                   f"Last sync: {self.last_sync_duration:.1f}s")

tracker = PerformanceTracker()

# Game state cache - track which games are finished to avoid re-processing
game_state_cache = {}  # {game_id: {"state": "FINAL", "last_check": timestamp}}

# Graceful shutdown flag
shutdown_requested = False

def signal_handler(signum, frame):
    """Handle shutdown signals gracefully"""
    global shutdown_requested
    shutdown_requested = True
    logger.info("üõë Shutdown signal received, finishing current sync...")

# Register signal handlers (works on Unix/Linux, safe on Windows)
try:
    signal.signal(signal.SIGINT, signal_handler)
    signal.signal(signal.SIGTERM, signal_handler)
except Exception:
    pass  # Windows may not support all signals

# --- PARALLEL API CALLER (OPTIMIZED FOR IP REUSE) ---
def safe_api_call(url: str, max_retries: int = 3, reuse_session: bool = False) -> Optional[Dict[Any, Any]]:
    """
    Make API call using 100-IP rotation (no artificial delays needed).
    citrus_request handles rate limiting via proxy rotation.
    
    Args:
        url: URL to fetch
        max_retries: Number of retry attempts
        reuse_session: If True, tries to reuse same IP (for batch calls)
    """
    for attempt in range(max_retries):
        try:
            r = citrus_request(url, timeout=15)  # 100-IP proxy rotation
            if r.status_code == 429:
                wait = (attempt + 1) * 10
                logger.warning(f"üõë [429 LIMIT] Resting {wait}s...")
                time.sleep(wait)
                continue
            r.raise_for_status()
            return r.json()
        except Exception as e:
            if attempt < max_retries - 1:
                time.sleep(2)  # Brief retry delay
            else:
                logger.error(f"‚ùå API Error after {max_retries} attempts: {e}")
    return None

# --- IP-EFFICIENT BATCH CALLER ---
def safe_api_call_batch(urls: List[str], max_retries: int = 3) -> List[Optional[Dict[Any, Any]]]:
    """
    Make multiple API calls reusing the same IP (when possible).
    Reduces IP usage from N calls = N IPs to N calls = 1 IP.
    
    Args:
        urls: List of URLs to fetch
        max_retries: Number of retry attempts
        
    Returns:
        List of responses (None for failed calls)
    """
    results = []
    
    # Import here to avoid circular dependency
    from src.utils.citrus_request import get_proxy_manager
    proxy_manager = get_proxy_manager()
    
    # Get ONE proxy for all calls in this batch
    proxy_url = proxy_manager.get_next_proxy()
    
    if not proxy_url:
        # Fallback: No proxy available, use individual calls
        logger.warning("‚ö†Ô∏è No proxy available, falling back to individual calls")
        return [safe_api_call(url, max_retries) for url in urls]
    
    # Use same proxy for all URLs in batch
    proxies = {"http": proxy_url, "https": proxy_url}
    proxy_ip = proxy_url.split('@')[1].split(':')[0] if '@' in proxy_url else proxy_url.split(':')[0]
    
    for url in urls:
        response_data = None
        for attempt in range(max_retries):
            try:
                url_display = url if len(url) <= 60 else f"...{url[-57:]}"
                logger.info(f"[Batch-Call] Requesting {url_display} via {proxy_ip}...")
                
                r = requests.get(url, proxies=proxies, timeout=15)
                
                if r.status_code == 429:
                    wait = (attempt + 1) * 10
                    logger.warning(f"üõë [429 LIMIT] Resting {wait}s...")
                    time.sleep(wait)
                    continue
                    
                r.raise_for_status()
                response_data = r.json()
                logger.info(f"[Batch-Call] ‚úÖ Success (200)")
                break
                
            except Exception as e:
                if attempt < max_retries - 1:
                    time.sleep(2)
                else:
                    logger.error(f"‚ùå Batch call error: {e}")
        
        results.append(response_data)
    
    return results

# --- PROCESS SINGLE GAME (FOR PARALLEL EXECUTION) ---
def process_single_game(game_id: str, game_date: str) -> Dict[str, Any]:
    """
    Process a single game (PBP + Boxscore + Stats).
    Returns: {"game_id": ..., "state": ..., "success": bool, "details": ...}
    """
    details = {"pbp": False, "raw_ingest": False, "boxscore": False, "stats": False}
    
    # SMART CACHING: Skip FINAL games (with TTL for stat corrections)
    cached = game_state_cache.get(game_id)
    if cached and cached["state"] in ("FINAL", "OFF"):
        # Check TTL: Re-verify FINAL games every 2 hours for first 24h (catch stat corrections)
        cache_age = time.time() - cached["last_check"]
        cache_ttl = 7200  # 2 hours in seconds
        max_age = 86400   # 24 hours - after this, trust it's truly final
        
        if cache_age < max_age:
            # Game is fresh-ish, but check if TTL expired
            if cache_age < cache_ttl:
                # Cache is still valid - use it!
                return {
                    "game_id": game_id, 
                    "state": cached["state"], 
                    "success": True, 
                    "details": {"cached": True},
                    "cached": True
                }
            # TTL expired but <24h old - re-fetch to catch stat corrections
            logger.info(f"   [Game {game_id}] Re-checking FINAL game for stat corrections...")
        else:
            # Game is >24h old - it's truly final, cache forever
            return {
                "game_id": game_id, 
                "state": cached["state"], 
                "success": True, 
                "details": {"cached": True},
                "cached": True
            }
    
    try:
        # OPTIMIZATION: Fetch PBP and Boxscore with same IP (1 IP per game instead of 2!)
        # This reduces IP usage by 50% while maintaining data quality
        
        pbp_url = f"https://api-web.nhle.com/v1/gamecenter/{game_id}/play-by-play"
        box_url = f"https://api-web.nhle.com/v1/gamecenter/{game_id}/boxscore"
        
        # Fetch both PBP and Boxscore with same IP
        batch_results = safe_api_call_batch([pbp_url, box_url])
        pbp = batch_results[0] if batch_results else None
        box = batch_results[1] if len(batch_results) > 1 else None
        
        if not pbp:
            logger.warning(f"   [Game {game_id}] Failed to fetch PBP data")
            return {"game_id": game_id, "state": "ERROR", "success": False, "details": details}
        
        details["pbp"] = True
        state = pbp.get("gameState", "").upper()
        
        # Update cache with current state
        game_state_cache[game_id] = {"state": state, "last_check": time.time()}
        
        # 2. Ingest Raw PBP (for xG processing later)
        try:
            from ingest_live_raw_nhl import upsert_raw_game, supabase_client
            upsert_raw_game(supabase_client(), game_id, game_date, pbp)
            details["raw_ingest"] = True
        except Exception as e:
            logger.warning(f"   [Game {game_id}] Raw ingest error (non-critical): {e}")
            # Non-critical - continue processing
        
        # 3. Process Stats (if game is active/final and we got boxscore)
        if state in ("LIVE", "CRIT", "OFF", "FINAL", "INTERMISSION"):
            if box:
                details["boxscore"] = True
                try:
                    from scrape_live_nhl_stats import process_game_data_citrus
                    process_game_data_citrus(game_id, box, pbp)
                    details["stats"] = True
                    return {"game_id": game_id, "state": state, "success": True, "details": details}
                except Exception as e:
                    logger.error(f"   [Game {game_id}] Stats processing error: {e}")
                    return {"game_id": game_id, "state": state, "success": False, "details": details}
            else:
                logger.warning(f"   [Game {game_id}] Failed to fetch boxscore")
                return {"game_id": game_id, "state": state, "success": False, "details": details}
        
        # Scheduled game - no stats to process yet
        return {"game_id": game_id, "state": state, "success": True, "details": details}
        
    except Exception as e:
        logger.error(f"   [Game {game_id}] Unexpected error: {e}")
        return {"game_id": game_id, "state": "ERROR", "success": False, "details": details}

# --- THE UNIFIED LOOP ---
def run_unified_loop() -> Tuple[str, int]:
    """
    Main sync loop - processes all today's games in parallel.
    Returns: (game_state, live_count) for adaptive scheduling
    """
    sync_start = time.time()
    logger.info("=" * 60)
    logger.info(f"üöÄ SYNC START - {dt.datetime.now().strftime('%H:%M:%S')}")
    
    try:
        # 1. Get Schedule from DB (Source of Truth)
        from supabase_rest import SupabaseRest
        db = SupabaseRest(os.getenv("VITE_SUPABASE_URL"), os.getenv("SUPABASE_SERVICE_ROLE_KEY"))
        today = dt.date.today().isoformat()
        games = db.select("nhl_games", filters=[("game_date", "eq", today)])
        
        if not games:
            logger.warning("‚ö†Ô∏è No games found in DB schedule.")
            # Clear cache if no games today
            if game_state_cache:
                game_state_cache.clear()
            tracker.total_syncs += 1
            tracker.last_sync_duration = time.time() - sync_start
            return ("OFF_HOURS", 0)  # Return state info for smart scheduling

        # Smart cache management: Remove cached games not in today's schedule
        game_ids_today = {g['game_id'] for g in games}
        stale_cache_keys = [gid for gid in game_state_cache.keys() if gid not in game_ids_today]
        if stale_cache_keys:
            for gid in stale_cache_keys:
                del game_state_cache[gid]
            logger.info(f"üóëÔ∏è Cleared {len(stale_cache_keys)} stale cache entries")

        logger.info(f"üìã Found {len(games)} games in slate. Processing ALL IN PARALLEL...")
    except Exception as e:
        logger.error(f"‚ùå CRITICAL: Failed to fetch schedule from DB: {e}")
        tracker.failed_syncs += 1
        return ("ERROR", 0)

    # 2. PARALLEL PROCESSING - Hit all games at once with 100 IPs!
    # Use ThreadPoolExecutor to process all games simultaneously
    results = []
    live_count = 0
    success_count = 0
    
    try:
        with ThreadPoolExecutor(max_workers=min(len(games), 20)) as executor:
            # Submit all games for parallel processing
            future_to_game = {
                executor.submit(process_single_game, g['game_id'], today): g
                for g in games
            }
            
            # Collect results as they complete
            for future in as_completed(future_to_game):
                game = future_to_game[future]
                try:
                    result = future.result()
                    results.append(result)
                    tracker.games_processed += 1
                    
                    # Track success/failure
                    if result.get("success"):
                        success_count += 1
                    else:
                        tracker.games_failed += 1
                    
                    # Log result
                    state = result.get("state", "UNKNOWN")
                    success = "‚úÖ" if result.get("success") else "‚ùå"
                    is_cached = result.get("cached", False)
                    cached_tag = " [CACHED]" if is_cached else ""
                    
                    if state in ("LIVE", "CRIT"):
                        live_count += 1
                        logger.info(f"üî¥ LIVE: [{result['game_id']}] {success}")
                    elif state == "INTERMISSION":
                        logger.info(f"‚è∏Ô∏è  INT: [{result['game_id']}] {success}")
                    elif state in ("FINAL", "OFF"):
                        logger.info(f"üèÅ FINAL: [{result['game_id']}] {success}{cached_tag}")
                    else:
                        logger.info(f"üìÖ {state}: [{result['game_id']}] {success}")
                        
                except Exception as e:
                    logger.error(f"‚ùå Game {game['game_id']} failed: {e}")
                    tracker.games_failed += 1
        
        # All games processed in parallel! Total time = slowest game (not sum of all games)
        # Count cached vs fresh API calls
        cached_count = sum(1 for r in results if r.get("cached", False))
        fresh_count = len(games) - cached_count
        ips_used = fresh_count  # Each fresh game = 1 IP (batch call optimization!)
        ips_saved = cached_count
        
        logger.info(f"üìä BATCH COMPLETE: {success_count}/{len(games)} games successful "
                   f"({cached_count} cached, {fresh_count} fresh) | "
                   f"üí∞ IPs: {ips_used} used, {ips_saved} saved")
        
    except Exception as e:
        logger.error(f"‚ùå CRITICAL: Parallel processing failed: {e}")
        tracker.failed_syncs += 1
        return ("ERROR", 0)

    # 3. Matchup Refresh
    try:
        from calculate_matchup_scores import update_active_matchup_scores
        update_active_matchup_scores(db)
        logger.info("üèÜ [MATCHUPS] Scoreboard Balanced.")
    except Exception as e:
        logger.error(f"‚ö†Ô∏è Matchup update failed (non-critical): {e}")
    
    # 4. Determine game state from results
    game_states = [r.get("state", "SCHEDULED") for r in results]
    all_cached = all(r.get("cached", False) for r in results)
    
    if any(s in ("LIVE", "CRIT") for s in game_states):
        game_state = "LIVE"
    elif any(s == "INTERMISSION" for s in game_states):
        game_state = "INTERMISSION"
    elif all(s in ("FINAL", "OFF") for s in game_states):
        # ALL games finished - check if we can extend sleep even more
        if all_cached:
            # All games cached (within 2h TTL) - can sleep longer
            game_state = "ALL_FINAL_CACHED"
        else:
            # Some games needed re-check (TTL expired) - normal interval
            game_state = "ALL_FINAL"
    else:
        game_state = "SCHEDULED"
    
    # 5. NIGHTLY PBP AUDIT - FIXED: Must run BEFORE return statement!
    now = dt.datetime.now()
    if now.hour == 23 and now.minute >= 50:
        logger.info("üåô END OF NIGHT DETECTED. Starting Deep PBP Audit...")
        try:
            from run_daily_pbp_processing import process_all_unprocessed_games
            process_all_unprocessed_games()
            logger.info("‚úÖ Nightly PBP processing complete.")
        except Exception as e: 
            logger.error(f"‚ùå Nightly PBP Error: {e}")

    # 6. NIGHTLY LANDING STATS UPDATE (PPP/SHP Season Totals)
    # Run at midnight MT (00:00-00:05) to update season totals after all games are final
    if now.hour == 0 and now.minute < 5:
        logger.info("üåô MIDNIGHT MT - Starting Nightly Landing Stats Update (PPP/SHP)...")
        try:
            from fetch_nhl_stats_from_landing import main as fetch_landing_stats
            result = fetch_landing_stats()
            if result == 0:
                logger.info("‚úÖ Nightly landing stats update complete.")
            else:
                logger.error(f"‚ùå Nightly landing stats update failed with code {result}")
        except Exception as e:
            logger.error(f"‚ùå Nightly Landing Stats Error: {e}")

    # Track performance metrics
    tracker.total_syncs += 1
    tracker.last_sync_duration = time.time() - sync_start
    
    # Periodic health check (every 10 syncs)
    if tracker.total_syncs % 10 == 0:
        tracker.log_health_check()
    
    logger.info("=" * 60)
    
    return (game_state, live_count)

if __name__ == "__main__":
    # BOOT MESSAGE - Verify this in your terminal
    print("\n" + "‚ñà" * 70)
    print("‚ñà" + " " * 68 + "‚ñà")
    print("‚ñà   üçã CITRUS MASTER - PARALLEL MODE (30s BULLETPROOF)           ‚ñà")
    print("‚ñà   Architecture: 100-IP Auto-Rotation + Parallel Processing      ‚ñà")
    print("‚ñà   Features: ALL games hit simultaneously, ZERO rate limits      ‚ñà")
    print("‚ñà   Performance: McDavid scores ‚Üí 30-35s to your app (3x faster) ‚ñà")
    print("‚ñà" + " " * 68 + "‚ñà")
    print("‚ñà" * 70 + "\n")
    
    consecutive_failures = 0
    max_consecutive_failures = 5

    while not shutdown_requested:
        try:
            game_state, live_count = run_unified_loop()
            consecutive_failures = 0  # Reset on success
            
            # Check for shutdown request after sync
            if shutdown_requested:
                break
            
            # ADAPTIVE SCHEDULING: Use game state to determine refresh rate
            # With 100 IPs, we can be MUCH more aggressive during live action
            now = dt.datetime.now()
            is_game_hours = 17 <= now.hour <= 23  # 5pm-11pm MT
            
            # LIVE GAME MODE - Ultra-safe aggressive refresh (30 seconds)
            if game_state == "LIVE" and live_count > 0:
                sleep_time = 30  # üî• 30s refresh - bulletproof against rate limits!
                logger.info(f"üî¥ {live_count} LIVE GAMES - Aggressive Mode (30s refresh)...")
            
            # INTERMISSION MODE - Moderate refresh (60 seconds)
            elif game_state == "INTERMISSION" and is_game_hours:
                sleep_time = 60  # Games on break, check every minute
                logger.info("‚è∏Ô∏è  Intermission - checking every 60s...")
            
            # ALL FINAL CACHED MODE - All games cached within TTL (30 minutes)
            elif game_state == "ALL_FINAL_CACHED":
                sleep_time = 1800  # 30 minutes - all cached, minimal monitoring
                logger.info("‚úÖ All games FINAL (all cached) - extended sleep (30 min)...")
            
            # ALL FINAL MODE - Some games needed TTL refresh (10 minutes)
            elif game_state == "ALL_FINAL":
                sleep_time = 600  # 10 minutes - checking for stat corrections
                logger.info("‚úÖ All games FINAL - checking for stat corrections (10 min)...")
            
            # SCHEDULED MODE - Games haven't started yet (2 minutes)
            elif game_state == "SCHEDULED" and is_game_hours:
                sleep_time = 120  # Check every 2 min for game start
                logger.info("üìÖ Pre-game - checking every 2 min...")
            
            # ERROR MODE - Back off exponentially
            elif game_state == "ERROR":
                sleep_time = min(300, 30 * (2 ** consecutive_failures))  # Max 5 min
                logger.warning(f"‚ö†Ô∏è ERROR recovery mode - waiting {sleep_time}s...")
            
            # OFF HOURS - Save bandwidth (5 minutes)
            else:
                sleep_time = 300  # 5 minutes when no games
                logger.info("üò¥ Off hours - resting 5 min to save bandwidth...")
            
            time.sleep(sleep_time)
            
        except KeyboardInterrupt:
            logger.info("üõë Shutdown requested by user...")
            logger.info(f"üìä Final stats: {tracker.total_syncs} syncs, {tracker.games_processed} games processed")
            tracker.log_health_check()
            sys.exit(0)
            
        except Exception as e:
            consecutive_failures += 1
            logger.error(f"‚ùå FATAL ERROR ({consecutive_failures}/{max_consecutive_failures}): {e}")
            
            if consecutive_failures >= max_consecutive_failures:
                logger.critical("üÜò TOO MANY CONSECUTIVE FAILURES! Service requires attention!")
                logger.critical("Service will continue but may be degraded...")
                consecutive_failures = 0  # Reset to avoid spam
            
            # Exponential backoff on errors
            backoff = min(300, 30 * (2 ** (consecutive_failures - 1)))
            logger.info(f"Backing off {backoff}s before retry...")
            time.sleep(backoff)